{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Задача 1**.\n",
        "\n",
        " Пусть $f(X) = \\ln \\det X,$ где $X \\in \\mathbb{R}^{n\\times n}$. Найдите производную $\\nabla_X f(X)$.\n",
        "\n",
        "**Решение**.  Функция $f(X) = \\ln \\det X $, где $ X \\in \\mathbb{R}^{n\\times n} $.\n",
        "\n",
        "Чтобы найти градиент $f(X)$, обозначенный как $ \\nabla_X f(X) $, сначала выразим $f(X)$ через след матрицы $ X $ и его обратную матрицу $X^{-1} $.\n",
        "\n",
        "Используя свойство логарифма детерминанта и следа матрицы, мы можем записать:\n",
        "$$ f(X) = \\ln \\det X = \\ln e^{\\text{tr}(\\ln X)} = \\text{tr}(\\ln X) $$\n",
        "\n",
        "Теперь найдем градиент функции $f(X)$. Для этого сначала найдем производную по элементам матрицы $X$:\n",
        "$$ \\frac{\\partial f(X)}{\\partial X_{ij}} = \\frac{\\partial}{\\partial X_{ij}} \\text{tr}(\\ln X) $$\n",
        "\n",
        "Затем, используя свойство следа матрицы, получаем:\n",
        "$$ \\frac{\\partial f(X)}{\\partial X_{ij}} = [\\exp(\\ln X)]_{ji} = (X^{-1})_{ji} $$\n",
        "\n",
        "Таким образом, градиент функции $ f(X) = \\ln \\det X $по матрице $ X $ равен:\n",
        "$$ \\nabla_X f(X) = (X^{-1})^T $"
      ],
      "metadata": {
        "id": "vxNBva3yP_Vp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Задача 2.**\n",
        "\n",
        "Пусть $f(x) = x^T \\exp(xx^T)x$, где $x \\in \\mathbb{R}^{n}$, а $\\exp(B)-$\n",
        "[матричная экспонента](https://en.wikipedia.org/wiki/Matrix_exponential)\n",
        ",\n",
        " $B \\in \\mathbb{R}^{n \\times n}$.\n",
        "Матричной экспонентой обозначают ряд\n",
        "$$\n",
        "        I_n + \\frac{B}{1!} + \\frac{B^2}{2!} + \\frac{B^3}{3!} + \\frac{B^4}{4!} + \\ldots = \\sum_{k=0}^\\infty \\frac{B^k}{k!} .\n",
        "$$\n",
        "Найдите производную $\\nabla_x f(x)$.\n",
        "\n",
        "\n",
        "\n",
        "**Решение** Функция $$ f(x) = x^T \\exp(xx^T)x, $$ где $ x \\in \\mathbb{R}^{n} $, а $ \\exp(B) $ - матричная экспонента, где $$ B \\in \\mathbb{R}^{n \\times n}. $$\n",
        "\n",
        "Матричные экспоненты определяются рядом\n",
        "    $$\n",
        "        I_n + \\frac{B}{1!} + \\frac{B^2}{2!} + \\frac{B^3}{3!} + \\frac{B^4}{4!} + \\ldots = \\sum_{k=0}^\\infty \\frac{B^k}{k!} .\n",
        "$$\n",
        "Найдем производную $f(x) $, обозначенную как $\\nabla_x f(x).$\n",
        "\n",
        "Для начала найдем \\( \\nabla_x f(x) \\):\n",
        "$$\n",
        "\\begin{aligned}\n",
        "f(x) &= x^T \\exp(xx^T)x \\\\\n",
        "&= x^T \\left( I_n + \\frac{xx^T}{1!} + \\frac{(xx^T)^2}{2!} + \\frac{(xx^T)^3}{3!} + \\ldots \\right) x \\\\\n",
        "&= x^T \\left( I_n + x(x^T)x + \\frac{x(x^T)x(x^T)x}{2!} + \\frac{x(x^T)x(x^T)x(x^T)x}{3!} + \\ldots \\right) x \\\\\n",
        "&= x^T \\left( I_n + xx^Tx + \\frac{(x^Tx)^2x}{2!} + \\frac{(x^Tx)^3x}{3!} + \\ldots \\right) \\\\\n",
        "&= x^T (I_n + 2xx^Tx + 3(x^Tx)^2x + \\ldots) \\\\\n",
        "&= x^T (I_n + 2xx^Tx + 3\\|x\\|^2x + \\ldots) \\\\\n",
        "&= x^T (I_n + 2xx^Tx + 3\\|x\\|^2x + \\ldots) \\\\\n",
        "&= x + 2\\|x\\|^2x + 3\\|x\\|^4x + \\ldots \\\\\n",
        "&= x(1 + 2\\|x\\|^2 + 3\\|x\\|^4 + \\ldots).\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "Таким образом, $$\\nabla_x f(x) = 1 + 2\\|x\\|^2 + 3\\|x\\|^4 + \\ldots $$."
      ],
      "metadata": {
        "id": "lHG1J8AgOhGL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "7ub6wgk_SYCG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Задача 3**. Пусть $A \\in \\mathbb{R}^{m \\times n}, b \\in \\mathbb{R}^m, x \\in \\mathbb{R}^n$. Найдите производную $\\nabla_x f(x)$ функции\n",
        "$$\n",
        "  f(x) = \\sin \\|Ax + b\\|_2\n",
        "$$\n",
        "\n",
        "**Рещшение.** Для функции $ f(x) = \\sin \\|Ax + b\\|_2 $, где $ A \\in \\mathbb{R}^{m \\times n} $, $ b \\in \\mathbb{R}^m $, и $ x \\in \\mathbb{R}^n $, найдем градиент по переменной $x $, обозначаемый как $\\nabla_x f(x)$.\n",
        "\n",
        "Сначала выразим функцию  $f(x)$ через квадрат нормы вектора $ Ax + b $:\n",
        "$$ f(x) = \\sin \\|Ax + b\\|_2 = \\sin \\sqrt{(Ax + b)^T(Ax + b)} = \\sin \\sqrt{(x^TA^TAx + 2b^TAx + b^Tb)}\n",
        "$$\n",
        "\n",
        "Теперь найдем градиент функции $f(x)$ по переменной $x$. Для этого сначала найдем производную подкоренного выражения:\n",
        "$$ \\frac{\\partial}{\\partial x} \\|Ax + b\\|_2 = \\frac{1}{\\|Ax + b\\|_2} (A^T(Ax + b)) $$\n",
        "\n",
        "Используя цепное правило дифференцирования для функции синуса, получаем:\n",
        "$$ \\nabla_x f(x) = \\cos(\\|Ax + b\\|_2) \\cdot \\frac{1}{\\|Ax + b\\|_2} (A^T(Ax + b)) = \\frac{\\cos(\\|Ax + b\\|_2)}{\\|Ax + b\\|_2} A^T(Ax + b) $$\n",
        "\n",
        "Таким образом, градиент функции $ f(x) = \\sin \\|Ax + b\\|_2 $ по переменной $ x $ равен:\n",
        "$$\\nabla_x f(x) = \\frac{\\cos(\\|Ax + b\\|_2)}{\\|Ax + b\\|_2} A^T(Ax + b) $$"
      ],
      "metadata": {
        "id": "1RDubUDKQlQ4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Задача 4.  Рассмотрим симметричную матрицу $A \\in \\mathbb{R}^{n \\times n}$ и ее спектральное разложение $A = Q \\Lambda Q^T$.\n",
        "\n",
        " Пусть $\\lambda \\in \\mathbb{R}^n$ - это диагональ матрицы $\\Lambda$ (то есть вектор, составленный из собственных значений $A$).\n",
        "\n",
        "Найдите производные:\n",
        "\n",
        "- $\\displaystyle \\nabla_\\lambda Tr(A)$\n",
        "- $\\displaystyle \\nabla_Q Tr(A)$\n",
        "   \n",
        "\n",
        "**Решение.**\n",
        "\n",
        "Для нахождения производных трассы матрицы $ A$ по вектору $ \\lambda $ и матрице $ Q $, воспользуемся свойствами следа матрицы и матричного дифференцирования.\n",
        "\n",
        "1. Найдем производную $ \\nabla_\\lambda \\text{Tr}(A) $:\n",
        "$$\\nabla_\\lambda \\text{Tr}(A) = \\nabla_\\lambda \\text{Tr}(Q\\Lambda Q^T) = \\nabla_\\lambda \\text{Tr}(\\Lambda) = 1 $$\n",
        "\n",
        "2. Найдем производную $ \\nabla_Q \\text{Tr}(A)$:\n",
        "$$ \\nabla_Q \\text{Tr}(A) = \\nabla_Q \\text{Tr}(Q\\Lambda Q^T) = \\nabla_Q \\text{Tr}(\\Lambda Q^T Q) = \\nabla_Q \\text{Tr}(\\Lambda) = Q^T $$\n",
        "\n",
        "Таким образом, производные будут:\n",
        "- $ \\nabla_\\lambda \\text{Tr}(A) = 1 $\n",
        "- $\\nabla_Q \\text{Tr}(A) = Q^T $"
      ],
      "metadata": {
        "id": "MYI_guGRTzTm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Задачу 5**. Рассмотрим задачу обучения линейной регрессии с функцией потерь Log-Cosh:\n",
        "$$\n",
        "Q(w) = \\frac{1}{\\ell} \\sum_{i=1}^{\\ell} \\ln (\\cosh (w^T x_i - y_i))\n",
        "$$\n",
        " Выпишите формулу для градиента $\\nabla_w Q(w)$. Запишите ее в матричном виде, используя матрицу объекты-признаки $X$ и вектор целевых переменных $y$.\n",
        "\n",
        "**Решение.** Для функции потерь Log-Cosh в задаче обучения линейной регрессии:\n",
        "$$ Q(w) = \\frac{1}{\\ell} \\sum_{i=1}^{\\ell} \\ln (\\cosh (w^T x_i - y_i)) $$\n",
        "\n",
        "Формула для градиента $ \\nabla_w Q(w) $ будет:\n",
        "$$ \\nabla_w Q(w) = \\frac{1}{\\ell} \\sum_{i=1}^{\\ell} \\frac{\\sinh(w^T x_i - y_i)}{\\cosh(w^T x_i - y_i)} x_i $$\n",
        "\n",
        "Теперь запишем эту формулу в матричном виде, используя матрицу объекты-признаки $X $ и вектор целевых переменных $ y$:\n",
        "$$ \\nabla_w Q(w) = \\frac{1}{\\ell} X^T \\cdot \\text{diag}\\left(\\frac{\\sinh(Xw - y)}{\\cosh(Xw - y)}\\right) $$\n",
        "\n",
        "Здесь:\n",
        "- $ X $ - матрица объекты-признаки размером $ \\ell \\times d $, где $ \\ell $ - количество объектов, $ d $ - количество признаков,\n",
        "- $ y $- вектор целевых переменных размером $ \\ell \\times 1 $,\n",
        "- $ w $ - вектор весов размером $ d \\times 1 $$,\n",
        "- функция $ \\text{diag}(\\cdot) $ создает диагональную матрицу из вектора.\n",
        "\n",
        "Таким образом, градиент $ \\nabla_w Q(w) $ в матричном виде будет выражаться как указано выше."
      ],
      "metadata": {
        "id": "XYriaoaHUlwi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Задача 6.** В случае одномерной Ridge-регрессии минимизируется функция со штрафом:\n",
        "$$\n",
        "Q(w) = (y - xw)^T(y - xw) + \\lambda w^2,\n",
        "$$\n",
        "где $\\lambda$ — положительный параметр, штрафующий функцию за слишком большие значения $w$.\n",
        "    \n",
        "\n",
        "* Найдите производную $\\nabla_w Q(w)$, выведите формулу для оптимального $w$.\n",
        "* Найдите вторую производную $\\nabla_w^2 Q(w)$. Убедитесь, что мы оказались в точке минимума.\n",
        "* Выпишите шаг градиентного спуска в матричном виде.\n",
        "\n",
        "**Решение.** Для одномерной Ridge-регрессии минимизируется функция с штрафом:\n",
        "\\[ Q(w) = (y - xw)^T(y - xw) + \\lambda w^2 \\]\n",
        "\n",
        "1. Найдем производную $\\nabla_w Q(w)$:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "Q(w) &= (y - xw)^T(y - xw) + \\lambda w^2 \\\\\n",
        "&= y^Ty - 2w^Tx^Ty + w^Tx^Txw + \\lambda w^2 \\\\\n",
        "\\end{aligned}\n",
        "$$\n",
        "Тогда\n",
        "$$\n",
        "\\nabla_w Q(w) = -2x^Ty + 2x^Txw + 2\\lambda w\n",
        "$$\n",
        "\n",
        "2. Найдем вторую производную $ \\nabla_w^2 Q(w) $:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\nabla_w^2 Q(w) &= 2x^Tx + 2\\lambda\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "Так как $\\nabla_w^2 Q(w) $ положительно определена (так как $x^Tx $ всегда неотрицательно), то мы находимся в точке минимума.\n",
        "\n",
        "3. Шаг градиентного спуска в матричном виде:\n",
        "$$ w_{\\text{новый}} = w_{\\text{старый}} - \\alpha \\nabla_w Q(w_{\\text{старый}}) $$\n",
        "где $ \\alpha $ - размер шага.\n"
      ],
      "metadata": {
        "id": "wg00hYf3WIM0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Задача 7.**  Найдите симметричную матрицу $X$, наиболее близкую к матрице $A$ по норме Фробениуса\n",
        "\n",
        "~$$\\sum_{i,j} (x_{ij} - a_{ij})^2$$.\n",
        "\n",
        "Иными словами, решите задачу условной матричной минимизации\n",
        "\\begin{equation*}\n",
        "  \\begin{cases}\n",
        "  &||X - A||_F^2 \\to \\min_{X}  \\\\\n",
        "  &X^T = X\n",
        "  \\end{cases}\n",
        "\\end{equation*}\n",
        "\n",
        "**На заметку:** Надо будет выписать лагранжиан.  А ещё пригодится тот факт, что $$\\sum_{i,j} (x_{ij} - a_{ij})^2 = ||X - A||_F^2 =  Tr((X-A)^T (X-A))$$.\n",
        "\n",
        "**Решение.** Для нахождения симметричной матрицы $ X $, наиболее близкой к матрице $A$ по норме Фробениуса, мы можем решить задачу условной матричной минимизации с использованием метода множителей Лагранжа.\n",
        "\n",
        "Дано:\n",
        "$$ ||X - A||_F^2 = Tr((X-A)^T (X-A)) $$\n",
        "\n",
        "Сформулируем задачу оптимизации с учетом ограничения $X^T = X $:\n",
        "$$ \\min_X Tr((X-A)^T (X-A)) + \\lambda Tr(X^T - X) $$\n",
        "\n",
        "Где $ \\lambda$ - множитель Лагранжа.\n",
        "\n",
        "Найдем производную Лагранжиана по матрице $ X $ и приравняем к нулю:\n",
        "$$ \\frac{\\partial}{\\partial X} \\left( Tr((X-A)^T (X-A)) + \\lambda Tr(X^T - X) \\right) = 0 $$\n",
        "\n",
        "Решив это уравнение, получим оптимальное значение матрицы $X$, которое будет наиболее близким к матрице $ A $ по норме Фробениуса и при этом будет симметричным."
      ],
      "metadata": {
        "id": "WjwMmXnmXpZy"
      }
    }
  ]
}