{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Как правило, дифференцируемые модели обучаются с помощью градиентного спуска, а для него важно уметь считать градиент функционала ошибки по параметрам модели. Можно считать градиент покоординатно, а потом пристально смотреть на формулы и пытаться понять, как это может выглядеть в векторной форме. Гораздо проще считать градиент напрямую --- а для этого поможет знание градиентов для основных функций и основных правил матрично-векторного дифференцирования.\n",
        "\n",
        "##Вывод основных формул\n",
        "\n",
        "Для начала вспомним определение производной для обычной функции одной переменной.\n",
        "Говорят, что у функции $f: R \\to R$ в точке $x$ есть производная $f'(x)$,\n",
        "если в этой точке функция представима в следующем виде для всех достаточно маленьких $\\mathrm{d}x$:\n",
        "$$\n",
        "    f(x + \\mathrm{d}x)\n",
        "    =\n",
        "    f(x)\n",
        "    +\n",
        "    f'(x) \\mathrm{d}x\n",
        "    +\n",
        "    o(\\mathrm{d}x).\n",
        "$$\n",
        "То есть производная --- это коэффициент, определяющий линейную часть приращения функции.\n",
        "Обобщим это на случай функций, работающих в конечномерных линейных пространствах с нормами.\n",
        "Говорят, что функция $f: R^n \\to R^m$ дифференцируема в точке $x$,\n",
        "если существует такой линейный оператор $L: R^n \\to R^m$, что для любых достаточно\n",
        "малых по норме $\\mathrm{d}x \\in R^n$ выполнено\n",
        "$$\n",
        "    f(x + \\mathrm{d}x)\n",
        "    =\n",
        "    f(x)\n",
        "    +\n",
        "    L[\\mathrm{d}x]\n",
        "    +\n",
        "    o(\\|\\mathrm{d}x\\|).\n",
        "$$\n",
        "Можно показать, что если функция дифференцируема в точке $x$,\n",
        "то соответствующий линейный оператор определяется единственным образом.\n",
        "Будем обозначать его как $\\mathrm{d}f(x)$ и называть дифференциалом.\n",
        "Поскольку дифференциал --- это линейный оператор, то в зависимости от размерностей\n",
        "пространств он может быть представлен как скалярное произведение $\\langle a_x, dx{x} \\rangle$,\n",
        "умножение на матрицу $A_x dx{x}$ или схожим образом.\n",
        "Тогда по аналогии с одномерным случаем вектор $a_x$ или матрица $A_x$ являются <<производными>>\n",
        "функции в точке $x$.\n",
        "Как мы знаем, для них есть специальные названия: $a_x$ называется градиентом, $A_x$ --- матрицей Якоби."
      ],
      "metadata": {
        "id": "MQZoR6iwBxml"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Когда мы работали с одномерными функциями, в большинстве случаев для поиска производных нам хватало небольшой таблицы со стандартными случаями и пары правил.\n",
        "Для случая векторных и матричных функций все эти правила можно обобщить, а таблицы дополнить специфическими функциями вроде определителя.\n",
        "Удобнее всего оказывается работать в терминах «дифференциала» --- с ним можно не задумываться о промежуточных размерностях, а просто применять стандартные правила.\n",
        "\n",
        "Введём некоторые обозначения:\n",
        "\n",
        "* При отображении вектора в число $f(x): R^n \\to R$\n",
        "$$\n",
        "\\nabla_x f(x)\n",
        "   =\n",
        "        \\bigg[\n",
        "            \\frac{\\partial f}{\\partial x_1},\n",
        "            \\dots,\n",
        "            \\frac{\\partial f}{\\partial x_n}\n",
        "        \\bigg]^T\n",
        "$$\n",
        "\n",
        "* При отображении матрицы в число $f(A): R^{n \\times m} \\to R$\n",
        "$$\n",
        "\\nabla_A f(A)\n",
        "        =\n",
        "        \\bigg(\n",
        "            \\frac{\\partial f}{\\partial A_{ij}}\n",
        "        \\bigg)_{i,j=1}^{n,m}\n",
        "$$\n",
        "    \n",
        "* При отображении вектора в вектор $f(x): R^n \\to R^m$\n",
        "$$\n",
        "  \\mathfrak{J}_x = \\bigg(\n",
        "   \\frac{\\partial f_{i}}{\\partial x_{j}}\n",
        "   \\bigg)_{i,j=1}^{n,m}\n",
        "$$\n"
      ],
      "metadata": {
        "id": "Kyp8Plr8Cgbc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Мы хотим оценить, как функция изменяется по каждому из аргументов по отдельности. Поэтому производной функции по вектору будет вектор, по матрице --- матрица.\n",
        "\n",
        "Нарисуем таблицу с тем, как выглядят дифференциалы для разных случаев. По строчкам будем откладывать то, откуда бьёт функция, то есть входы. По столбцам будем откладывать то, куда бьёт функция, то есть выходы. На пересечении будут расположены дифференциалы. Для ситуаций, обозначенных прочерками, обобщения получить не выйдет.\n"
      ],
      "metadata": {
        "id": "2Gywh9fqEmQx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "WCwTr9WhE877"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Всегда, когда мы будем сталкиваться с дифференцированием на практике, мы будем выяснять, к какой из ситуации относится задача, а дальше сводит дифференциал к виду из таблицы выше и вытаскивать из него производную.\n",
        "\n",
        "%Дифференциал --- это линейная часть приращения функции. Если мы находимся в какой-то точке $x_0$ и делаем из неё небольшое приращение $dx{x},$ то наша функция изменится примерно на $dx{f(x)}$.\n",
        "\n",
        "\n",
        "*  Для функции, которая бьёт \\textbf{из скаляров в скаляры} $f(x) : R \\to R$ дифференциал выглядит как $dx{f(x)} = f'(x) dx{x}$.\n",
        "\n",
        "*  Когда функция бьёт \\textbf{из векторов в скаляры} $f(x) : R^n \\to R.$, мы имеем дело с функцией нескольких переменных. Нам нужно взять производную по каждой из них и получить вектор производных, градиент\n",
        "\n",
        "$$\n",
        "\\nabla_x f = \\begin{pmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\ldots  \\\\ \\frac{\\partial f}{\\partial x_n} \\end{pmatrix}\n",
        "$$\n",
        "\n",
        "Если умножить транспонированный градиент на вектор приращений, у нас получится дифференциал\n",
        "\n",
        "$$\n",
        "dx{f(x)} = \\nabla_x f^T dx{x} = \\begin{pmatrix} \\frac{\\partial f}{\\partial x_1}, & \\ldots,  & \\frac{\\partial f}{\\partial x_n} \\end{pmatrix} \\begin{pmatrix} dx{x_1} \\\\  \\ldots  \\\\ dx{x_n} \\end{pmatrix} = \\frac{\\partial f}{\\partial x_1} \\cdot dx{x_1} + \\ldots +\\frac{\\partial f}{\\partial x_n} \\cdot dx{x_n}.\n",
        "$$\n",
        "\n",
        "При изменении $x_i$ на $dx{x_i}$ функция будет при прочих равных меняться пропорционально соответствующей частной производной.\n",
        "\n",
        "* Когда функция бьёт \\textbf{из векторов в векторы} $f(x) : R^n \\to R^m$, мы взаимодействуем с семейством функций. Например, если $n=1$ то у нас есть $m$ функций, каждая из которых применяется к $x$. На выходе получается вектор\n",
        "\n",
        "$$\n",
        "\\begin{pmatrix} f_1(x) \\\\ f_2(x) \\\\ \\ldots  \\\\ f_m(x). \\end{pmatrix}\n",
        "$$\n"
      ],
      "metadata": {
        "id": "mIBtFbXQGPIL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Если мы хотим найти производную, нужно взять частную производную каждой функции по $x$ и записать в виде вектора. Дифференциал также будет представлять из себя вектор, так как при приращении аргумента на какую-то величину изменяется каждая из функций\n",
        "\n",
        "$$\n",
        "dx{f(x)} = \\begin{pmatrix} \\frac{\\partial f_1}{\\partial x} \\\\ \\frac{\\partial f_2}{\\partial x} \\\\ \\ldots  \\\\ \\frac{\\partial f_m}{\\partial x} \\end{pmatrix} \\cdot \\begin{pmatrix} dx{x}  \\end{pmatrix}  = \\begin{pmatrix} \\frac{\\partial f_1}{\\partial x} dx{x} \\\\ \\frac{\\partial f_2}{\\partial x} dx{x} \\\\ \\ldots  \\\\ \\frac{\\partial f_m}{\\partial x} dx{x} \\end{pmatrix}.\n",
        "$$\n",
        "\n",
        "Тут мы умножаем каждую строчку из вектора размера $n \\times 1$ на столбец матрицы $1 \\times 1$. Если хочется, можно рассуждать об этом как о поэлементном умножении. Если $n > 1$, то аргументов на вход в такой вектор из функций идёт несколько, на выходе получается матрица\n",
        "\n",
        "$$\n",
        "\\begin{pmatrix} f_1(x_1) & f_1(x_2) & \\ldots & f_1(x_n) \\\\ f_2(x_1)  & f_2(x_2) & \\ldots & f_2(x_n)  \\\\ \\ldots & \\ldots & \\ddots & \\ldots  \\\\ f_m(x_1)  & f_m(x_2) & \\ldots & f_m(x_n) \\end{pmatrix}\n",
        "$$\n",
        "  \n",
        "Производной такой многомерной функции будет матрица из частных производных каждой функции по каждому аргументу\n",
        "\n",
        "$$\n",
        "\\begin{pmatrix} \\frac{\\partial f_1}{\\partial x_1} & \\frac{\\partial f_1}{\\partial x_2} & \\ldots & \\frac{\\partial f_1}{\\partial x_n} \\\\ \\frac{\\partial f_2}{\\partial x_1}  & \\frac{\\partial f_2}{\\partial x_2} & \\ldots & \\frac{\\partial f_2}{\\partial x_n}  \\\\ \\ldots & \\ldots & \\ddots & \\ldots  \\\\ \\frac{\\partial f_m}{\\partial x_1}  & \\frac{\\partial f_m}{\\partial x_2} & \\ldots & \\frac{\\partial f_m}{\\partial x_n} \\end{pmatrix}.\n",
        "$$\n"
      ],
      "metadata": {
        "id": "pQwssUF7GhK2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Дифференциал снова будет представлять из себя вектор\n",
        "\n",
        "$$\n",
        "dx{f(x)} =  \\begin{pmatrix} \\frac{\\partial f_1}{\\partial x_1} & \\frac{\\partial f_1}{\\partial x_2} & \\ldots & \\frac{\\partial f_1}{\\partial x_n} \\\\ \\frac{\\partial f_2}{\\partial x_1}  & \\frac{\\partial f_2}{\\partial x_2} & \\ldots & \\frac{\\partial f_2}{\\partial x_n}  \\\\ \\ldots & \\ldots & \\ddots & \\ldots  \\\\ \\frac{\\partial f_m}{\\partial x_1}  & \\frac{\\partial f_m}{\\partial x_2} & \\ldots & \\frac{\\partial f_m}{\\partial x_n} \\end{pmatrix} \\cdot  \\begin{pmatrix} dx{x_1} \\\\ dx{x_2} \\\\ \\ldots  \\\\ dx{x_n} \\end{pmatrix} =  \\begin{pmatrix} \\frac{\\partial f_1}{\\partial x_1} dx{x_1} + \\frac{\\partial f_1}{\\partial x_2} dx{x_2} + \\ldots + \\frac{\\partial f_1}{\\partial x_n} dx{x_n}  \\\\ \\frac{\\partial f_2}{\\partial x_1} dx{x_1} +  \\frac{\\partial f_2}{\\partial x_2} dx{x_2} + \\ldots + \\frac{\\partial f_2}{\\partial x_n} dx{x_n}  \\\\ \\ldots  \\\\ \\frac{\\partial f_m}{\\partial x_1} dx{x_1} + \\frac{\\partial f_m}{\\partial x_2} dx{x_2} + \\ldots + \\frac{\\partial f_m}{\\partial x_n} dx{x_n} \\end{pmatrix}.\n",
        "$$\n",
        "\n",
        "* Функция бьёт \\textbf{из матриц в скаляры} $f(A) : R^{n \\times k} \\to R.$В таком случае нам надо найти производную функции по каждому элементу матрицы, то есть дифференциал будет выглядеть как\n",
        "\n",
        "$$\n",
        "dx{f(A)} = \\frac{\\partial f}{\\partial a_{11}} dx{a_{11}} + \\ldots + \\frac{\\partial f}{\\partial a_{nk}} dx{a_{nk}}.\n",
        "$$\n",
        "\n",
        "Его можно записать в компактном виде через след матрицы как\n",
        "$$\n",
        "dx{f(A)} = Tr(\\nabla_A f^T dx{A}),\n",
        "$$\n",
        "\n",
        "Вполне естественен вопрос --- а почему это можно записать именно так?  Давайте попробуем увидеть этот факт на каком-нибудь простом примере. Пусть у нас есть две матрицы\n",
        "\n",
        "$$\n",
        "A_{[2 \\times 3]} = \\begin{pmatrix} a_{11} & a_{12} & a_{13} \\\\ a_{21} & a_{22} & a_{23} \\end{pmatrix} \\qquad B_{[2 \\times 3]} = \\begin{pmatrix} b_{11} & b_{12} & b_{13} \\\\ b_{21} & b_{22} & b_{23} \\end{pmatrix}.\n",
        "$$\n",
        "\n",
        "Посмотрим на то, как выглядит $Tr(B^T dx{A})$. Как это ни странно, он совпадает с дифференциалом\n",
        "\n",
        "$$Tr(B^T dx{A}) = Tr \\left( \\begin{pmatrix} b_{11} & b_{21} \\\\ b_{12} & b_{22} \\\\ b_{13} &  b_{23} \\end{pmatrix} \\begin{pmatrix} dx a_{11} & dx a_{12} & dx a_{13} \\\\ dx a_{21} & dx a_{22} & dx a_{23} \\end{pmatrix} \\right),\n",
        "$$\n",
        "\n",
        "при произведении на выходе получаем матрицу размера $3 \\times 3$\n",
        "\n",
        "$$\n",
        "\\begin{pmatrix} b_{11} dx a_{11} +  b_{21} dx a_{21} & b_{11} dx a_{12} +  b_{21} dx a_{22} & b_{11} dx a_{13} +  b_{21} dx a_{23} \\\\ b_{12} dx a_{11} +  b_{22} dx a_{21} & b_{12} dx a_{12} +  b_{22} dx a_{22} & b_{12} dx a_{13} +  b_{22} dx a_{23} \\\\ b_{13} dx a_{11} +  b_{23} dx a_{21} & b_{13} dx a_{12} +  b_{23} dx a_{22} & b_{13} dx a_{13} +  b_{23} dx a_{23} \\end{pmatrix}.\n",
        "$$\n",
        "\n",
        "Когда мы берём её след, остаётся сумма элементов по диагонали. Это и есть требуемый дифференциал. Дальше мы периодически будем пользоваться таким приёмом.\n"
      ],
      "metadata": {
        "id": "6ucVoGLFGkxr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Например,  величину $||X-A||^2 = \\sum_{i,j} (x_{ij} - a_{ij})^2$ можно записать в матричном виде как  $Tr((X-A)^T (X-A)).$\n",
        "\n",
        "* В таблице осталось ещё несколько ситуаций, которые остались вне поля нашего зрения.  Например, давайте посмотрим на ситуацию когда отображение бьёт из матриц в векторы $f(A) : R^{n \\times k} \\to R^m.$\n",
        "\n",
        "Тогда $A$ матрица, а $f(A)$ вектор. Нам надо найти производную каждого элемента из вектора $f(A)$ по каждому элементу из матрицы $A$. Получается, что $\\frac{\\partial f}{\\partial A}$ --- это трёхмерная структура. Мы с такими ситуациями встречаться не будем, поэтому опустим их.\n",
        "\n",
        "\n",
        "Свойства матричных дифференциалов очень похожи на свойства обычных. Надо только не забыть, что мы работаем с матрицами.\n",
        "\n",
        "$$\n",
        " dx{(AB)} = dx{A}B + Adx{B}, \\quad dx{A}B \\ne Bdx{A}\n",
        " $$\n",
        "\n",
        " $$\n",
        "   dx{(\\alpha A + \\beta B)} = \\alpha dx{A} + \\beta dx{B}\n",
        " $$\n",
        "\n",
        " $$\n",
        "   dx{(A^T)} = (dx{A})^T\n",
        "  $$\n",
        "\n",
        "  $$\n",
        "     dx{C} = 0, \\quad C - \\text{матрица из констант}\n",
        "$$\n",
        "\n",
        "Чтобы доказать все эти свойства достаточно просто аккуратно расписать их. Кроме этих правил нам понадобится пара трюков по работе со скалярами. Если $s$ --- скаляр размера $1 \\times 1$, тогда $s^T = s$ и $Tr(s) = s$.\n",
        "\n",
        "С помощью этих преобразований мы будем приводить дифференциалы к каноническому виду и вытаскивать из них производные.\n",
        "\n"
      ],
      "metadata": {
        "id": "FdmQ3n-sF0Rh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Задача 1.**  Пусть $a \\in \\mathbb{R}^n$ --- вектор параметров, а $x \\in \\mathbb{R}^n$ --- вектор переменных. Рассмотрим функцию, которая представляет из себя их скалярное произведение $f(x) = a^T x$. Нужно найти её производную по вектору переменных  $\\nabla_x f(x)$.\n",
        "\n",
        "\n",
        "**Решение**\n",
        "Функция $f(x)$ бьёт из векторов в скаляры $f(x) : R^n \\to R.$ Если мы хотим найти производную функции $f(x_1, x_2, \\ldots, x_n)$, нам надо взять производную по каждому аргументу и выписать градиент. Можно расписать умножение одного вектора на другой в виде привычной нам формулы\n",
        "\n",
        "$$\n",
        "\\underset{[1 \\times 1]}{f(x)} = \\underset{[1 \\times n]}{a^T} \\cdot \\underset{[n \\times 1]}{x} = \\begin{pmatrix} a_1 & a_2 & \\ldots &a_n \\end{pmatrix} \\cdot \\begin{pmatrix} x_1 \\\\ x_2 \\\\ \\ldots  \\\\ x_n \\end{pmatrix} = a_1 \\cdot x_1 + a_2 \\cdot x_2 + \\ldots + a_n \\cdot x_n.\n",
        "$$\n",
        "\n",
        "Из неё чётко видно, что $\\frac{\\partial f}{\\partial x_i} = a_i$\n",
        "\n",
        "$$\n",
        "\\nabla_x f = \\begin{pmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\\\ \\ldots  \\\\ \\frac{\\partial f}{\\partial x_n} \\end{pmatrix} = \\begin{pmatrix} a_1 \\\\ a_2 \\\\ \\ldots  \\\\ a_n \\end{pmatrix} = a,\n",
        "$$\n",
        "\n",
        "теперь можно записать дифференциал\n",
        "\n",
        "$$\n",
        "dx{f} = a^T dx{x} = \\frac{\\partial f}{\\partial x_1} \\cdot dx{x_1} + \\ldots +\\frac{\\partial f}{\\partial x_n} \\cdot dx{x_n} = a_1 \\cdot dx{x_1} + \\ldots + a_n \\cdot dx{x_n}.\n",
        "$$\n",
        "\n",
        "В то же самое время можно было бы просто воспользоваться правилами нахождения матричных дифференциалов\n",
        "\n",
        "$$\n",
        "dx{f} =  dx{a^T x} = a^T dx{x} = \\nabla_x f^T dx{x},\n",
        "$$\n",
        "\n",
        "откуда $ \\nabla_x f = a$. При таком подходе нам не надо анализировать каждую частную производную по отдельности. Мы находим все производные за раз.\n"
      ],
      "metadata": {
        "id": "NeI-uAmyG7ga"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Задача 2** Пусть $f(x) = x^T A x$, где $x$ вектор размера $n \\times 1$, $A$ матрица размера $n \\times n$. Найдите производную  $\\nabla_x f(x)$.\n",
        "\n",
        "\n",
        "**Решение**\n",
        "Функция бьёт из векторов в скаляры. Попробуем перемножить все матрицы и расписать её в явном виде по аналогии со скалярным произведением\n",
        "\n",
        "$$\n",
        "    \\underset{[1 \\times 1]}{f(x)} = \\underset{[1 \\times n]}{x^T} \\cdot \\underset{[n \\times n]}{A} \\cdot \\underset{[n \\times 1]}{x} = \\sum_{i = 1}^n \\sum_{j=1}^n a_{ij} \\cdot x_i \\cdot x_j.\n",
        "$$\n",
        "\n",
        "Если продолжить в том же духе, мы сможем найти все частные производные, а потом назад вернём их в матрицу. Однако это неудобно. Всё было записано в красивом компактном матричном виде, а мы это испортили. Более того, если множителей будет больше, тогда суммы станут совсем громоздкими, и мы легко запутаемся.\n",
        "\n",
        "При этом, если воспользоваться правилами работы с матричными дифференциалами, мы легко получим ответ\n",
        "\n",
        "$$\n",
        "dx{f} =  dx{x^T A x} =  dx{(x^T)} A x + x^T dx{(Ax)} =  dx{(x^T)} A x + x^T \\underset{dx{A} = 0}{dx{(A)}} x + x^T A dx{(x)}.\n",
        "$$\n",
        "\n",
        "Заметим, что $dx{(x^T)} A x$ это скаляр. Его транспонирование никак не повлияет на результат\n",
        "$$\n",
        "dx{f} = dx{(x^T)} A x + x^T A dx{(x)} = x^T A^T dx{x}  + x^T A dx{x} = x^T(A^T + A) dx{x}.\n",
        "$$\n",
        "\n",
        "Мы нашли матричный дифференциал и свели его к каноничной форме\n",
        "\n",
        "$$\n",
        "dx{f} = \\nabla^T_x f dx{x} = x^T(A^T + A) dx{x}\n",
        "$$\n",
        "\n",
        "Получается, что искомая производная $\\nabla_x f = (A + A^T) x$. Обратите внимание, что размерности не нарушены, и мы получили столбец из производных, то есть искомый градиент нашей функции $f(x)$. \\end{esSolution}\n"
      ],
      "metadata": {
        "id": "H9qCAPEzG-4H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Задача  3** Пусть $f(x) = x^T A x$, где $x \\in R^n, A \\in R^{n \\times n}.$ Найдите вторую производную по $x$.\n",
        "\n",
        "**Решение** Чтобы найти вторую производную, надо продифференцировать первую производную. Первая производная $g(x) = (A + A^T) x$ бьёт из векторов в векторы. Приведём дифференциал к каноническому виду\n",
        "\n",
        "$$\n",
        "dx{g(x)} = dx (A + A^T)x = (A + A^T) dx{x}.\n",
        "$$\n",
        "\n",
        "Выходит, что матрица из вторых производных для функции $f(x)$ выглядит как $A + A^T.$ Обратите внимание, что для этой ситуации в каноническом виде нет транспонирования.\n",
        "\n"
      ],
      "metadata": {
        "id": "No02KvDrHN6V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**Задача 4** Пусть $f(X) = a^TXAXa$, где $a \\in R^n, X \\in R^{n\\times n}.$ Необходимо найти производную $\\nabla_X f$.\n",
        "\n",
        "\n",
        "\n",
        "**Решение.**\n",
        "Функция бьёт из матриц в скаляры. Дифференциал будет по своей размерности совпадать со скаляром. Производная будет размера матрицы\n",
        "\n",
        "$$\n",
        "dx{f(X)} = dx{(a^TXAXa)} = a^Tdx{(X)}AXa + a^TXAdx{(X)}a.\n",
        "$$\n",
        "\n",
        "Оба слагаемых, которые мы получаем после перехода к дифференциалу --- скаляры. Мы хотим представить дифференциал в виде $Tr(\\text{нечто} dx{X})$. След скаляра --- это снова скаляр. Получается, что мы бесплатно можем навесить над правой частью нашего равенство знак следа и воспользоваться его свойствами\n",
        "\n",
        "\\begin{multline*}\n",
        "dx{f(X)} = dx{(a^TXAXa)} = Tr(a^Tdx{(X)}AXa) + Tr(a^TXAdx{(X)}a) = \\\\ = Tr(AXaa^Tdx{(X)}) + Tr(aa^TXAdx{(X)}) = \\\\ = Tr(AXaa^Tdx{(X)} + aa^TXAdx{(X)}) = Tr((AXaa^T + aa^TXA)dx{(X)}).\n",
        "\\end{multline*}\n",
        "\n",
        "Производная найдена, оказалось что это\n",
        "\n",
        "$$\n",
        "\\nabla_X f = (AXaa^T + aa^TXA)^T = aa^TX^TA^T + A^TX^Taa^T.\n",
        "$$"
      ],
      "metadata": {
        "id": "lWrbFedaHZlJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Задача 5.** Пусть $f(x) = x x^T x$, где $x \\in R^n.$ Необходимо найти производную $\\nabla_x f$.\n",
        "\n",
        "\n",
        "\n",
        "**Решение** Функция бьёт из векторов в векторы.\n",
        "\n",
        "$$\n",
        "\\underset{[n \\times 1]}{f(x)} = \\underset{[n \\times 1]}{x} \\underset{[1 \\times n]}{x^T}  \\underset{[n \\times 1]}{x}.\n",
        "$$\n",
        "\n",
        "Берём дифференциал\n",
        "\n",
        "$$\n",
        "dx{f(x)} = dx{xx^Tx} = dx{x}x^Tx + x dx{x^T} x + xx^Tdx{x}.\n",
        "$$\n",
        "\n",
        "В первом слагаемом пользуемся тем, что $x^Tx$ скаляр и его можно вынести перед дифференциалом. Этот скаляр умножается на каждый элемент вектора. Дальше мы захотим вынести дифференциал за скобку, чтобы не испортить матричное сложение, подчеркнём факт этого перемножения на каждый элемент единичной матрицей. Во втором слагаемом пользуемся тем, что $dx{x^T} x$ скаляр и транспонируем его\n",
        "$$\n",
        "dx{f(x)} = \\underset{[1 \\times 1]}{x^Tx} \\underset{[n \\times n]}{I_n} \\underset{[n \\times 1]}{dx{x}} + x x^T dx{x} + xx^Tdx{x} = (x^Tx I_n + 2 x x^T)dx{x}.\n",
        "$$\n",
        "\n",
        "Обратите внимание, что без единичной матрицы размерности у сложения поломаются. Получается, что наша производная выглядит как $\\mathfrak{J} = x^Tx I_n + 2 x x^T$\n",
        "\n",
        "\n",
        "Найдём несколько табличных производных, которыми мы дальше будем активно пользоваться: производную обратной матрицы, определителя и следа.\n"
      ],
      "metadata": {
        "id": "myDRm5GEHklw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Задача 6.** Пусть  $f(A) = A^{-1},$ где $A \\in \\mathbb{R}^{n\\times n}.$  Необходимо найти $\\nabla_A f(A)$.\n",
        "\n",
        "**Решение**\n",
        "Вспомним, что производная константы равна нолю. Обратная матрица определяется как $A^{-1} \\cdot A = I_n,$ где $I_n$ --- единичная матрица. Берём дифференциал с обеих сторон нашего равенства\n",
        "\n",
        "$$\n",
        "dx{A^{-1}} A + A^{-1} dx{A} = dx{I_n} = 0,\n",
        "$$\n",
        "\n",
        "отсюда получаем что $dx{A^{-1}} = - A^{-1} dx{A} A^{-1}.$ Везде, где мы будем встречать дифференциал обратной матрицы, мы будем использовать это значение.  Обратите внимание, что обратная матрица как функция отображает матрицы в матрицы. Эта клетка в табличке производных осталась нами незаполненной. Тем не менее мы можем использовать ту же самую технику с дифференциалами. Если где-то нам надо будет посчитать дифференциал обратной матрицы, мы будем поставлять туда полученную выше формулу.\n",
        "\n"
      ],
      "metadata": {
        "id": "3N2ZUJolHxw-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Задача 7.** Пусть $A \\in \\mathbb{R}^{n\\times n}$. Необходимо найти $\\nabla_A \\det A$.\n",
        "\n",
        "\n",
        "**Решение** Определитель --- это функция, которая бьёт из матриц в скаляры. Воспользуемся теоремой Лапласа о разложении определителя по строке:\n",
        "    $$\\frac{\\partial}{\\partial A_{ij}} \\det A = \\frac{\\partial}{\\partial A_{ij}}\\bigg[\\sum_k (-1)^{i+k}A_{ik}M_{ik}\\bigg] = (-1)^{i+j}M_{ij}, \\; $$\n",
        "где $M_{ik}$ --- дополнительный минор матрицы $A$. Также вспомним формулу для элементов обратной матрицы\n",
        "    \n",
        " $$(A^{-1})_{ij} = \\frac{1}{\\det A}(-1)^{i+j}M_{ji}.$$\n",
        "    \n",
        "Подставляя выражение для дополнительного минора, получаем ответ $\\nabla_A \\det A = (\\det A) A^{-T}$. При этом, так как функция бьёт из матриц в скаляры дифференциал можно записать как  $dx{ \\det A} = Tr(\\det (A) A^{-1} dx{A}).$\n",
        "\n"
      ],
      "metadata": {
        "id": "TKLv8VuOHtlF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Задача 8** Пусть $A \\in \\mathbb{R}^{n\\times n}$. Необходимо найти $\\nabla_A Tr(A)$.\n",
        "\n",
        "**Решение**\n",
        "По аналогии с определителем след бьёт из пространства матриц в пространство скаляров представляет из себя сумму диагональных элементов. Получается, что $dx{(Tr A)} = Tr(I_n dx A)$ и $\\nabla_A Tr A = I_n.$\n"
      ],
      "metadata": {
        "id": "qbVI3GWCIa9k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "   \n",
        "**Задача 9**  Пусть $A \\in \\mathbb{R}^{n \\times n},\\ B \\in \\mathbb{R}^{n \\times n}$. Необходимо найти $\\nabla_A \\text{tr}(AB)$.\n",
        "\n",
        "\n",
        "**Решение**\n",
        "Воспользовавшись циклическим свойством следа матрицы (для матриц подходящего размера):\n",
        "\n",
        "$$\n",
        "\\text{tr}(ABC) = \\text{tr}(BCA) = \\text{tr}(CAB)\n",
        "$$\n",
        "\n",
        "получаем\n",
        "\n",
        "$$\n",
        "dx{Tr(AB)} = Tr(dx{AB}) = Tr(Bdx{A}),\n",
        "$$\n",
        "\n",
        "то есть $\\nabla_A \\text{tr}(AB) = B^T.$"
      ],
      "metadata": {
        "id": "mi0c2zBwIbhR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Задача 10**  Пусть $x \\in \\mathbb{R}^n, \\, A \\in \\mathbb{R}^{n \\times m}, \\, y \\in \\mathbb{R}^m.$ Необходимо найти $\\nabla_A Tr(x^TAy)$.\n",
        "\n",
        "\n",
        " **Решение** Воспользовавшись циклическим свойством следа и результатом предыдущей задачи, получаем\n",
        "\n",
        "$$\n",
        "dx{Tr(x^TAy)} = Tr(dx{x^TAy}) = Tr(yx^Tdx{A}),\n",
        "$$\n",
        "\n",
        "то есть $\\nabla_A Tr(x^TAy) = xy^T.$\n"
      ],
      "metadata": {
        "id": "GTYBlADfIe2Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Наконец, научимся считать градиенты для сложных функций.\n",
        "Допустим, даны функции $f: R^n \\to R^m$ и $g: R^m \\to R$.\n",
        "Тогда градиент их композиции можно вычислить как\n",
        "$$\n",
        "    \\nabla_x g \\left( f(x) \\right)\n",
        "    =\n",
        "    \\mathfrak{J}_{f}^T (x)\n",
        "    \\nabla_z \\left. g(z) \\right|_{z = f(x)},\n",
        "$$\n",
        "где $\\mathfrak{J}_f (x) = \\left( \\frac{\\partial f_i(x)}{\\partial x_j}  \\right)_{i, j = 1}^{m, n}$ --- матрица Якоби для функции $f$.\n",
        "Если $m = 1$ и функция $g(z)$ имеет всего один аргумент, то формула упрощается:\n",
        "$$\n",
        "    \\nabla_x g \\left( f(x) \\right)\n",
        "    =\n",
        "    g'(f(x))\n",
        "    \\nabla_x f(x).\n",
        "$$\n",
        "\n"
      ],
      "metadata": {
        "id": "9F3b9EAJIoLf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Задача 11**\n",
        "    Вычислите градиент логистической функции потерь для линейной модели по параметрам этой модели:\n",
        "    $$\n",
        "        \\nabla_w\n",
        "        \\log \\left(\n",
        "            1\n",
        "            +\n",
        "            \\exp(-y \\langle w, x \\rangle)\n",
        "        \\right).\n",
        "    $$\n",
        "\n",
        "\n",
        " **Решение*\n",
        "Воспользуемся правилом взятия производной сложной функции и производной скалярного произведение из задачи выше\n",
        "\n",
        "\\begin{align*}\n",
        "    \\nabla_w \\log &\\left(\n",
        "        1 + \\exp(-y \\langle w, x \\rangle)\n",
        "    \\right) = \\\\\n",
        "    &= \\frac{1}{1 + \\exp(-y \\langle w, x \\rangle)} \\nabla_w \\left( 1 + \\exp(-y \\langle w, x \\rangle) \\right) =\\\\\n",
        "    &= \\frac{1}{1 + \\exp(-y \\langle w, x \\rangle)} \\exp(-y \\langle w, x \\rangle) \\nabla_w \\left( -y \\langle w, x \\rangle \\right) =\\\\\n",
        "    &= -\\frac{1}{1 + \\exp(-y \\langle w, x \\rangle)} \\exp(-y \\langle w, x \\rangle) y x =\\\\\n",
        "    &= \\left\\{ \\sigma(z) = \\frac{1}{1 + \\exp(-z)} \\right\\} =\\\\\n",
        "    &= -\\sigma(-y \\langle w, x \\rangle) y x\n",
        "\\end{align*}"
      ],
      "metadata": {
        "id": "vQQnvhhqIrf1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "### Решение задачи регрессии для многомерного случая\n",
        "\n",
        "Вспомним, зачем мы хотели научиться дифференцировать. В общем случае мы имеем выборку $\\{(x_i, y_i)\\}_{i=1}^\\ell$, $x_i \\in \\mathbb{R}^d, y_i \\in \\mathbb{R} \\; i = \\overline{1, \\ell}$, и хотим найти наилучшие параметры модели $a(x) = \\langle w, x \\rangle$ с точки зрения минимизации функции ошибки\n",
        "\n",
        "$$Q(w) = (y - Xw)^T(y - Xw).$$\n",
        "\n",
        "Здесь $X\\in \\mathbb{R}^{\\ell \\times d}$ — матрица <<объекты-признаки>>\\ для обучающей выборки, $y \\in \\mathbb{R}^\\ell$ — вектор значений целевой переменной на обучающей выборке, $w \\in \\mathbb{R}^d$ — вектор параметров. Выпишем дифференциал функции ошибки по $w$:\n",
        "\n",
        "\\begin{multline*}\n",
        "dx_w Q = dx_w[(y - Xw)^T(y - Xw)] = \\\\ = dx_w[(y - Xw)^T] (y - Xw) + (y - Xw)^Tdx_w[(y - Xw)] = \\\\ = dx_w[(-Xw)^T] (y - Xw) - (y - Xw)^TXdx w = \\\\ = - dx w^T X^T (y - Xw) - (y - Xw)^TXdx w = -2 (y - Xw)^TX dx w.\n",
        "\\end{multline*}\n",
        "\n",
        "Тут мы воспользовались тем, что $ dx w^T X^T (y - Xw)$ это скаляр и его можно транспонировать. Приравняем производную к нулю, чтобы найти минимум для $w$. Получается система уравнений\n",
        "\n",
        "$$\n",
        "2X^T(y-Xw) = 0 \\quad \\Rightarrow  \\quad  X^Ty = X^TX w \\quad \\Rightarrow  \\quad w = (X^TX)^{-1} X^Ty.\n",
        "$$\n",
        "\n",
        "При решении системы мы сделали предположение, что матрица $X^TX$ обратима. Это так, если в матрице $X$ нет линейно зависимых столбцов, а также наблюдений больше чем переменных.\n",
        "\n",
        "Заметим, что это общая формула, и нет необходимости выводить формулу для регрессии вида $a(x) = Xw + w_0$, т.к. мы всегда можем добавить признак (столбец матрицы $X$), который всегда будет равен $1$, и по уже выведенной формуле найдём параметр $w_0$.\n",
        "\n",
        "Если бы аналитического решения не существовало, мы могли бы найти точку оптимума с помощью градиентного спуска. Его шаг выглядел бы как\n",
        "\n",
        "$$\n",
        "w_t = w_{t-1} + \\gamma \\cdot 2X^T(y-Xw), \\quad \\text{здесь $\\gamma$ --- это скорость обучения.}\n",
        "$$\n",
        "\n",
        "Покажем, почему найденная точка — точка минимума, если матрица $X^T X$ обратима. Из курса математического анализа мы знаем, что если матрица Гессе функции положительно определёна в точке, градиент которой равен нулю, то эта точка является локальным минимумом. Найдём вторую производную\n",
        "\n",
        "$$\n",
        "dx_w[-2X^T(y-Xw)] = 2X^TX dx w.\n",
        "$$\n",
        "\n",
        "Выходит, что\n",
        "\n",
        "$$\n",
        "\\nabla^2 Q(w) = 2X^TX.\n",
        "$$\n",
        "\n",
        "Необходимо понять, является ли матрица $X^TX$ положительно определённой. Запишем определение положительной определённости матрицы  $X^TX$:\n",
        "\n",
        "$$\n",
        "z^TX^TXz > 0, \\; \\forall z \\in \\mathbb{R}^d, z \\ne 0.\n",
        "$$\n",
        "\n",
        "Видим, что тут записан квадрат нормы вектора $Xz$, то есть это выражение будет не меньше нуля. В случае, если матрица $X$ имеет <<книжную>>\\ ориентацию (строк не меньше, чем столбцов) и имеет полный ранг (нет линейно зависимых столбцов), то вектор $Xz$ не может быть нулевым, а значит выполняется\n",
        "\n",
        "$$\n",
        "z^TX^TXz = ||Xz||^2 > 0, \\; \\forall z \\in \\mathbb{R}^d, z \\ne 0.\n",
        "$$\n",
        "\n",
        "То есть $X^TX$ является положительно определённой матрицей. Также, по критерию Сильвестра, все главные миноры (в том числе и определитель) положительно определённой матрицы положительны, а, следовательно, матрица $X^TX$ обратима, и решение существует. Если же строк оказывается меньше, чем столбцов, или $X$ не является полноранговой, то $X^TX$ необратима и решение $w$ определено неоднозначно.\n"
      ],
      "metadata": {
        "id": "enWzixaoJLM-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "xsFYfr-QJTP6"
      }
    }
  ]
}